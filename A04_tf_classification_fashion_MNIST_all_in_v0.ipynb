{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Your Name and PRN:\n",
    "- Name: ______________________\n",
    "- PRN : ______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks \n",
    "## Session A04\n",
    "\n",
    "## Tensor Flow\n",
    "- Select number of layers\n",
    "- Select node in each of the layers\n",
    "- Choose activation function\n",
    "- multi-class - Fashion MNIST dataset (all 60000 images)\n",
    "- Implement one or more of following to achieve max accuracy.\n",
    "    - L1/ L2 Regularization\n",
    "    - Dropout\n",
    "    - Batch Normalization\n",
    "    - Early stopping\n",
    "- Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-13T01:28:03.750807Z",
     "iopub.status.busy": "2021-05-13T01:28:03.750218Z",
     "iopub.status.idle": "2021-05-13T01:28:09.430878Z",
     "shell.execute_reply": "2021-05-13T01:28:09.431273Z"
    },
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Global Variables\n",
    "###-----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Hyper Parameters\n",
    "###-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings so that Tensorflow can not Hog all the GPU memory\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print (f'Physical devices found : {physical_devices}')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------------\n",
    "### Function to plot Loss Curve\n",
    "###-----------------------------------\n",
    "\n",
    "def plot_tf_hist(hist_df):\n",
    "    \n",
    "    # instantiate figure\n",
    "    fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "    \n",
    "    # properties  matplotlib.patch.Patch \n",
    "    props = dict(boxstyle='round', facecolor='cyan', alpha=0.5)\n",
    "    fontsize=12\n",
    "    \n",
    "    # Where was min loss\n",
    "    best = hist_df[hist_df[hist_df.columns[2]] == hist_df[hist_df.columns[2]].min()]\n",
    "    \n",
    "    # pick first axis\n",
    "    ax = axes[0]\n",
    "    \n",
    "    y1 = hist_df.columns[0]\n",
    "    y2 = hist_df.columns[2]\n",
    "\n",
    "    # Plot all losses\n",
    "    hist_df.plot(y = [y1, y2], ax = ax)\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Loss: \\n  train: {:6.4f}\\n   test: {:6.4f}\"\n",
    "    txtstr = txtFmt.format(loss_df.iloc[-1][y1],\n",
    "                           loss_df.iloc[-1][y2]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.95, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Min: {best[y2].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y2].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy()-1, best[y2].to_numpy()[0]), # location of text \n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor='cyan', shrink=0.05)) # arrow\n",
    "    \n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3);\n",
    "    \n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title('Errors')\n",
    "    ax.grid()\n",
    "    ax.legend(loc = 'upper left') # model legend to upper left\n",
    "\n",
    "    # pick second axis\n",
    "    ax = axes[1]\n",
    "    \n",
    "    y1 = hist_df.columns[1]\n",
    "    y2 = hist_df.columns[3]\n",
    "\n",
    "    # Plot all losses\n",
    "    hist_df.plot(y = [y1, y2], ax = ax)\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Accuracy: \\n  train: {:6.4f}\\n  test:  {:6.4f}\"\n",
    "    txtstr = txtFmt.format(loss_df.iloc[-1][y1],\n",
    "                           loss_df.iloc[-1][y2]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.2, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Best: {best[y2].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y2].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy()-1, best[y2].to_numpy()[0]), # location of text \n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor='cyan', shrink=0.05)) # arrow\n",
    "    \n",
    "    \n",
    "     # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3);\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title('Accuracies')\n",
    "    ax.grid();\n",
    "    ax.legend(loc = 'lower left')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Load Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLdCchMdCaWQ"
   },
   "source": [
    "This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) datasetâ€”often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing you'll use here.\n",
    "\n",
    "This guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
    "\n",
    "Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and [load the Fashion MNIST data](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data) directly from TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-13T01:28:09.435454Z",
     "iopub.status.busy": "2021-05-13T01:28:09.434847Z",
     "iopub.status.idle": "2021-05-13T01:28:11.757368Z",
     "shell.execute_reply": "2021-05-13T01:28:11.757773Z"
    },
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "train_filename = os.path.join(inpDir, 'fashion_mnist', 'fashion-mnist_train.csv')\n",
    "test_filename = os.path.join(inpDir, 'fashion_mnist', 'fashion-mnist_test.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_filename, header = 0)\n",
    "test_df = pd.read_csv(test_filename, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9FDsUlxCaWW"
   },
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trouser</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-13T01:28:11.761874Z",
     "iopub.status.busy": "2021-05-13T01:28:11.761273Z",
     "iopub.status.idle": "2021-05-13T01:28:11.763330Z",
     "shell.execute_reply": "2021-05-13T01:28:11.762905Z"
    },
    "id": "IjnLH5S2CaWx"
   },
   "outputs": [],
   "source": [
    "#class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "class_names = {0: 'T-shirt/top',1:'Trouser',2:'Pullover',3:'Dress',4:'Coat',\n",
    "               5:'Sandal', 6: 'Shirt',7: 'Sneaker', 8:'Bag', 9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Brm0b_KACaWX"
   },
   "source": [
    "## Explore the data\n",
    "\n",
    "Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-13T01:28:11.767949Z",
     "iopub.status.busy": "2021-05-13T01:28:11.767358Z",
     "iopub.status.idle": "2021-05-13T01:28:11.770268Z",
     "shell.execute_reply": "2021-05-13T01:28:11.770636Z"
    },
    "id": "zW5k_xz1CaWX"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES6uQoLKCaWr"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz7l27Lz9S1P"
   },
   "source": [
    "Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the *training set* and the *testing set* be preprocessed in the same way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59veuiEZCaW4"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-13T01:28:12.959246Z",
     "iopub.status.busy": "2021-05-13T01:28:12.958646Z",
     "iopub.status.idle": "2021-05-13T01:28:14.576221Z",
     "shell.execute_reply": "2021-05-13T01:28:14.576664Z"
    },
    "id": "9ODch-OFCaW4"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ###------------------------\n",
    "    ### Declare your model here\n",
    "    ###------------------------\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic model instance\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKF6uW-BCaW-"
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------\n",
    "### Define call backs\n",
    "###------------------\n",
    "\n",
    "checkpoint_dir = os.path.join(modelDir, subDir)\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, altName)\n",
    "\n",
    "model_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                    monitor='val_loss',\n",
    "                                                    mode='auto',\n",
    "                                                    save_weights_only=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4P4zIV7E28Z"
   },
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-13T01:28:14.600451Z",
     "iopub.status.busy": "2021-05-13T01:28:14.599642Z",
     "iopub.status.idle": "2021-05-13T01:28:42.548298Z",
     "shell.execute_reply": "2021-05-13T01:28:42.548668Z"
    },
    "id": "xvwvpA64CaW_"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=[X_test, y_test],\n",
    "                    epochs=10,\n",
    "                    callbacks=[model_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(history.history)\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tf_hist(loss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCpr6DGyE28h"
   },
   "source": [
    "### Evaluate accuracy\n",
    "\n",
    "Next, compare how the model performs on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-13T01:28:42.552978Z",
     "iopub.status.busy": "2021-05-13T01:28:42.552358Z",
     "iopub.status.idle": "2021-05-13T01:28:43.051280Z",
     "shell.execute_reply": "2021-05-13T01:28:43.051656Z"
    },
    "id": "VflXLEeECaXC"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print('Test accuracy: {:5.2f}'.format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Above accuracy is post last iteration and not based on best weights.\n",
    "Best weights are saved in checkpoint during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "model1 = build_model()\n",
    "\n",
    "# Load the previously saved weights\n",
    "model1.load_weights(best)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model1.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
